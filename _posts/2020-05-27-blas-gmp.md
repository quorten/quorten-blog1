---
layout: post
title: "BLAS with GMP?  Yes, it was done..."
date: 2020-05-27 14:12 -0500
author: quorten
categories: [unlipic, media-prog]
tags: [unlipic, media-prog]
---

Alright, alright, looking for these better math libraries is getting
quite annoying.  Surely, if someone else hadn't made an integer
arithmetic version of BLAS, they'd made an arbitrary precision version
of BLAS?  Indeed, they _did_, it _was_ done in the past, but not
anymore.  The library I've found hasn't been updated in 8 years.
Also, you're asking about libraries using double-width intermediates
during multiplications?  Yes, indeed this MPACK library does make
reference to a floating point equivalent, QD/DD (quad-double,
double-double arithmetic).  Alas, that too is no longer maintained,
nor is the original webpage even on the Internet anymore!

Original BLAS, floating point only.

20200527/http://www.netlib.org/blas/

MPACK, arbitrary precision BLAS and LAPACK.

20200527/DuckDuckGo gmp blas  
20200527/http://mplapack.sourceforge.net/

QD/DD, quad-double, double-double floating point arithmetic.

20200527/https://web.archive.org/web/20100502022120/http://www.cs.berkeley.edu/~yozo/

<!-- more -->

Now, what about CGAL in the midst of all this?  CGAL touts using
arbitrary precision arithmetic for all operations.  Well, reading this
page on information about CGAL's philosophy on exact computation, I'm
finding some striking similarities between my own library and what
CGAL is doing internally.  Except, for CGAL, of course, they are doing
it _exclusively_ for geometric computation, to the exclusion of
exposing a general-purpose library for linear algebra.

20200527/https://www.cgal.org/exact.html

So, at the end of all of this, I can conclude that yes, there are a
variety of libraries that do things similar to the way I'm doing it in
my own library, but nothing quite fits the bill as nicely about what
I'm looking for as my own library does.

Also, there is something to be said about this kind of software
appearing in more commercial contexts as of late.  For many years,
this may have only been the interest of academia, but with the advent
of Tensor Processing Units (TPUs) using integer arithmetic extensively
to accelerate artificial intelligence computation and the like, there
is certainly a lot more commercial interest, and with that, the desire
to make the results of the development proprietary.  So that may
partially explain why we've been seeing such developments so publicly
available in the late 2000s, but then they suddenly dropped off a
cliff and fell into dis-maintenance after the year 2012.  Google
wanted to sweep up all of that math tech talent for themselves.
